# BSZ : 构建engine以及推理时的batchsize
# IMGSIZE : 模型输入hw大小
# RUN_MODE : [FPS, MAP]
# PRECISION : [float16, int8]
# MODEL_NAME : 生成onnx/engine的basename
# ORIGINE_MODEL : 原始onnx文件
# COCO_GT : COCOEVAL标签文件
# DATASET_DIR : 量化/推理数据集路径
# CHECKPOINTS_DIR : 存放生成的onnx/engine路径
# LAYER_FUSION : decoder部分走融合算子实现  0不融合 1融合
# DECODER_FASTER : 有两种融合实现,faster版本速度快且可以直接对接gpu nms;另一种实现的输出和onnx保持一致.  1:faster
IMGSIZE=640
MODEL_NAME=yolov5m
ORIGINE_MODEL=yolov5m.onnx
DATA_PROCESS_TYPE=yolov5
MODEL_INPUT_NAMES=(images)

LAYER_FUSION=1
DECODER_FASTER=1
DECODER_NUM_CLASS=80
DECODER_INPUT_NAMES=(/model.24/m.0/Conv_output_0 /model.24/m.1/Conv_output_0 /model.24/m.2/Conv_output_0)
DECODER_8_ANCHOR=(10 13 16 30 33 23)
DECODER_16_ANCHOR=(30 61 62 45 59 119)
DECODER_32_ANCHOR=(116 90 156 198 373 326)

# NMS CONFIG
    # IOU_THRESH : iou阈值
    # SCORE_THRESH : bbox置信度阈值
    # MAX_BOX_PRE_IMG : 每张图片预测bbox的数量上限
    # ALL_BOX_NUM : nms接收每张图片的box数量
    # NMS_TYPE : GPU/CPU(TODO)
IOU_THRESH=0.6
SCORE_THRESH=0.001
MAX_BOX_PRE_IMG=1000
ALL_BOX_NUM=25200
NMS_TYPE=GPU

# QUANT CONFIG (仅PRECISION为int8时生效)
    # QUANT_OBSERVER : 量化策略，可选 [hist_percentile, percentile, minmax, entropy, ema]
    # QUANT_BATCHSIZE : 量化时组dataloader的batchsize, 最好和onnx中的batchsize保持一致，有些op可能推导shape错误(比如Reshape)
    # QUANT_STEP : 量化步数
    # QUANT_SEED : 随机种子 保证量化结果可复现
    # QUANT_EXIST_ONNX : 如果有其他来源的量化模型则填写
QUANT_OBSERVER=hist_percentile
QUANT_BATCHSIZE=1
QUANT_STEP=32
QUANT_SEED=42
DISABLE_QUANT_LIST=()
QUANT_EXIST_ONNX=